# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lJGOjLmZzaPUgQ6VxODsf_hQ6XA4sYMr
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load the three files
file_1RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _201 _01_ RB_Hea.asc'
file_2RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _203 _02_ RB_Hea.asc'
file_3RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _205 _03_ RB_Hea.asc'
file_4RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _207 _04_ RB_Hea.asc'
file_5RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _209 _05_ RB_Hea.asc'
file_6RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _211 _06_ RB_Hea.asc'
file_7RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _213 _07_ RB_Hea.asc'
file_8RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _215 _08_ RB_Hea.asc'
file_9RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _217 _09_ RB_Hea.asc'
file_10RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _219 _10_ RB_Hea.asc'
file_11RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _221 _11_ RB_Hea.asc'
file_12RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _223 _12_ RB_Hea.asc'
file_13RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _225 _13_ RB_Hea.asc'
file_14RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _227 _14_ RB_Hea.asc'
file_15RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _229 _15_ RB_Hea.asc'
file_16RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _231 _16_ RB_Hea.asc'
file_17RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _233 _17_ RB_Hea.asc'
file_18RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _235 _18_ RB_Hea.asc'
file_19RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _237 _19_ RB_Hea.asc'
file_20RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _239 _20_ RB_Hea.asc'
file_21RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _241 _21_ RB_Hea.asc'
file_22RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _243 _22_ RB_Hea.asc'
file_23RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _245 _23_ RB_Hea.asc'
file_24RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _247 _24_ RB_Hea.asc'
file_25RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _249 _25_ RB_Hea.asc'
file_26RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _251 _26_ RB_Hea.asc'
file_27RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _253 _27_ RB_Hea.asc'
file_28RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _255 _28_ RB_Hea.asc'
file_29RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _257 _29_ RB_Hea.asc'
file_30RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _259 _30_ RB_Hea.asc'
file_31RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _261 _31_ RB_Hea.asc'
file_32RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _263 _32_ RB_Hea.asc'
file_33RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _265 _33_ RB_Hea.asc'
file_34RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _267 _34_ RB_Hea.asc'
file_35RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _269 _35_ RB_Hea.asc'
file_36RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _271 _36_ RB_Hea.asc'
file_37RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _273 _37_ RB_Hea.asc'
file_38RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _275 _38_ RB_Hea.asc'
file_39RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _277 _39_ RB_Hea.asc'
file_40RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _279 _40_ RB_Hea.asc'
file_41RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _281 _41_ RB_Hea.asc'
file_42RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _283 _42_ RB_Hea.asc'
file_43RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _285 _43_ RB_Hea.asc'
file_44RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _287 _44_ RB_Hea.asc'
file_45RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _289 _45_ RB_Hea.asc'
file_46RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _291 _46_ RB_Hea.asc'
file_47RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _293 _47_ RB_Hea.asc'
file_48RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _295 _48_ RB_Hea.asc'
file_49RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _297 _49_ RB_Hea.asc'
file_50RB_path = '/content/drive/MyDrive/EMGDataset/Healthy/EMG _299 _50_ RB_Hea.asc'



# Read the files into dataframes
df_1RB = pd.read_csv(file_1RB_path, delimiter='\t', header=None)
df_2RB = pd.read_csv(file_2RB_path, delimiter='\t', header=None)
df_3RB = pd.read_csv(file_3RB_path, delimiter='\t', header=None)
df_4RB = pd.read_csv(file_4RB_path, delimiter='\t', header=None)
df_5RB = pd.read_csv(file_5RB_path, delimiter='\t', header=None)
df_6RB = pd.read_csv(file_6RB_path, delimiter='\t', header=None)
df_7RB = pd.read_csv(file_7RB_path, delimiter='\t', header=None)
df_8RB = pd.read_csv(file_8RB_path, delimiter='\t', header=None)
df_9RB = pd.read_csv(file_9RB_path, delimiter='\t', header=None)
df_10RB = pd.read_csv(file_10RB_path, delimiter='\t', header=None)
df_11RB = pd.read_csv(file_11RB_path, delimiter='\t', header=None)
df_12RB = pd.read_csv(file_12RB_path, delimiter='\t', header=None)
df_13RB = pd.read_csv(file_13RB_path, delimiter='\t', header=None)
df_14RB = pd.read_csv(file_14RB_path, delimiter='\t', header=None)
df_15RB = pd.read_csv(file_15RB_path, delimiter='\t', header=None)
df_16RB = pd.read_csv(file_16RB_path, delimiter='\t', header=None)
df_17RB = pd.read_csv(file_17RB_path, delimiter='\t', header=None)
df_18RB = pd.read_csv(file_18RB_path, delimiter='\t', header=None)
df_19RB = pd.read_csv(file_19RB_path, delimiter='\t', header=None)
df_20RB = pd.read_csv(file_20RB_path, delimiter='\t', header=None)
df_21RB = pd.read_csv(file_21RB_path, delimiter='\t', header=None)
df_22RB = pd.read_csv(file_22RB_path, delimiter='\t', header=None)
df_23RB = pd.read_csv(file_23RB_path, delimiter='\t', header=None)
df_24RB = pd.read_csv(file_24RB_path, delimiter='\t', header=None)
df_25RB = pd.read_csv(file_25RB_path, delimiter='\t', header=None)
df_26RB = pd.read_csv(file_26RB_path, delimiter='\t', header=None)
df_27RB = pd.read_csv(file_27RB_path, delimiter='\t', header=None)
df_28RB = pd.read_csv(file_28RB_path, delimiter='\t', header=None)
df_29RB = pd.read_csv(file_29RB_path, delimiter='\t', header=None)
df_30RB = pd.read_csv(file_30RB_path, delimiter='\t', header=None)
df_31RB = pd.read_csv(file_31RB_path, delimiter='\t', header=None)
df_32RB = pd.read_csv(file_32RB_path, delimiter='\t', header=None)
df_33RB = pd.read_csv(file_33RB_path, delimiter='\t', header=None)
df_34RB = pd.read_csv(file_34RB_path, delimiter='\t', header=None)
df_35RB = pd.read_csv(file_35RB_path, delimiter='\t', header=None)
df_36RB = pd.read_csv(file_36RB_path, delimiter='\t', header=None)
df_37RB = pd.read_csv(file_37RB_path, delimiter='\t', header=None)
df_38RB = pd.read_csv(file_38RB_path, delimiter='\t', header=None)
df_39RB = pd.read_csv(file_39RB_path, delimiter='\t', header=None)
df_40RB = pd.read_csv(file_40RB_path, delimiter='\t', header=None)
df_41RB = pd.read_csv(file_41RB_path, delimiter='\t', header=None)
df_42RB = pd.read_csv(file_42RB_path, delimiter='\t', header=None)
df_43RB = pd.read_csv(file_43RB_path, delimiter='\t', header=None)
df_44RB = pd.read_csv(file_44RB_path, delimiter='\t', header=None)
df_45RB = pd.read_csv(file_45RB_path, delimiter='\t', header=None)
df_46RB = pd.read_csv(file_46RB_path, delimiter='\t', header=None)
df_47RB = pd.read_csv(file_47RB_path, delimiter='\t', header=None)
df_48RB = pd.read_csv(file_48RB_path, delimiter='\t', header=None)
df_49RB = pd.read_csv(file_49RB_path, delimiter='\t', header=None)
df_50RB = pd.read_csv(file_50RB_path, delimiter='\t', header=None)









# Concatenate the dataframes row-wise
concatenated_RBH = pd.concat([df_1RB, df_2RB, df_3RB, df_4RB, df_5RB, df_6RB, df_7RB, df_8RB
                             , df_9RB, df_10RB, df_11RB, df_12RB, df_13RB, df_14RB, df_15RB, df_16RB
                             , df_17RB, df_18RB, df_19RB, df_20RB, df_21RB, df_22RB, df_23RB, df_24RB
                             , df_25RB, df_26RB, df_27RB, df_28RB, df_29RB, df_30RB, df_31RB, df_32RB
                             , df_33RB, df_34RB, df_35RB, df_36RB, df_37RB, df_38RB, df_39RB, df_40RB
                             , df_41RB, df_42RB, df_43RB, df_44RB, df_45RB, df_46RB, df_47RB, df_48RB
                             , df_49RB, df_50RB], axis=0, ignore_index=True)

# Check the datatypes
print(concatenated_RBH.dtypes)

# Display the first few rows
concatenated_RBH

import pandas as pd



def read_asc_file(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()
        # Split each line into a list of floats
        data = [list(map(float, line.split())) for line in lines]
    return data

# Read the files
data_1RB = read_asc_file(file_1RB_path)
data_2RB = read_asc_file(file_2RB_path)
data_3RB= read_asc_file(file_3RB_path)
data_4RB= read_asc_file(file_4RB_path)
data_5RB= read_asc_file(file_5RB_path)
data_6RB= read_asc_file(file_6RB_path)
data_7RB= read_asc_file(file_7RB_path)
data_8RB= read_asc_file(file_8RB_path)
data_9RB= read_asc_file(file_9RB_path)
data_10RB= read_asc_file(file_10RB_path)
data_11RB= read_asc_file(file_11RB_path)
data_12RB= read_asc_file(file_12RB_path)
data_13RB= read_asc_file(file_13RB_path)
data_14RB= read_asc_file(file_14RB_path)
data_15RB= read_asc_file(file_15RB_path)
data_16RB= read_asc_file(file_16RB_path)
data_17RB= read_asc_file(file_17RB_path)
data_18RB= read_asc_file(file_18RB_path)
data_19RB= read_asc_file(file_19RB_path)
data_20RB= read_asc_file(file_20RB_path)
data_21RB= read_asc_file(file_21RB_path)
data_22RB= read_asc_file(file_22RB_path)
data_23RB= read_asc_file(file_23RB_path)
data_24RB= read_asc_file(file_24RB_path)
data_25RB= read_asc_file(file_25RB_path)
data_26RB= read_asc_file(file_26RB_path)
data_27RB= read_asc_file(file_27RB_path)
data_28RB= read_asc_file(file_28RB_path)
data_29RB= read_asc_file(file_29RB_path)
data_30RB= read_asc_file(file_30RB_path)
data_31RB= read_asc_file(file_31RB_path)
data_32RB= read_asc_file(file_32RB_path)
data_33RB= read_asc_file(file_33RB_path)
data_34RB= read_asc_file(file_34RB_path)
data_35RB= read_asc_file(file_35RB_path)
data_36RB= read_asc_file(file_36RB_path)
data_37RB= read_asc_file(file_37RB_path)
data_38RB= read_asc_file(file_38RB_path)
data_39RB= read_asc_file(file_39RB_path)
data_40RB= read_asc_file(file_40RB_path)
data_41RB= read_asc_file(file_41RB_path)
data_42RB= read_asc_file(file_42RB_path)
data_43RB= read_asc_file(file_43RB_path)
data_44RB= read_asc_file(file_44RB_path)
data_45RB= read_asc_file(file_45RB_path)
data_46RB= read_asc_file(file_46RB_path)
data_47RB= read_asc_file(file_47RB_path)
data_48RB = read_asc_file(file_48RB_path)
data_49RB = read_asc_file(file_49RB_path)
data_50RB = read_asc_file(file_50RB_path)

# Concatenate the data
all_data = (data_1RB + data_2RB + data_3RB + data_4RB + data_5RB + data_6RB + data_7RB + data_8RB + data_9RB + data_10RB + data_11RB + data_12RB + data_13RB + data_14RB +
              data_15RB + data_16RB + data_17RB + data_18RB + data_19RB + data_20RB + data_21RB + data_22RB + data_23RB + data_24RB + data_25RB + data_26RB +
            data_27RB + data_28RB + data_29RB + data_30RB + data_31RB + data_32RB + data_33RB + data_34RB + data_35RB + data_36RB + data_37RB +
            data_38RB + data_39RB + data_40RB + data_41RB + data_42RB + data_43RB + data_44RB + data_45RB + data_46RB + data_47RB + data_48RB + data_49RB + data_50RB


            )

# Convert to pandas DataFrame
df_RB = pd.DataFrame(all_data)

# Display the first few rows
df_RB

import pandas as pd
import numpy as np
# Save the cleaned data to a CSV file in the /content directory
local_file_path = '/content/df_RB.csv'
df_RB.to_csv(local_file_path, index=False)

print(f"Data cleaned and saved to '{local_file_path}'")

# Download the file
from google.colab import files
#files.download(local_file_path)

import pandas as pd
import numpy as np
from google.colab import files

# Load the dataset directly from the provided file path
file_path = '/content/df_RB.csv'
data = pd.read_csv(file_path)

# Display the first few rows to understand its structure
print("Original Data:")
print(data.head())

# Step 1: Calculate the mean of the dataset
mean_value = data.mean().mean()

# Step 2: Replace NaNs with the mean value
RBHdata_cleaned = data.fillna(mean_value)

# Step 3: Replace unwanted values (e.g., negative values) with the mean value
# Assuming negative values are unwanted
RBHdata_cleaned[RBHdata_cleaned < 0] = mean_value

# Display the cleaned data
print("Cleaned Data:")
print(RBHdata_cleaned.head())

# Save the cleaned data to a new CSV file in the /content directory
RBHcleaned_file_path = '/content/RBHcleaned_data.csv'
RBHdata_cleaned.to_csv(RBHcleaned_file_path, index=False)

print(f"Cleaned data saved to '{RBHcleaned_file_path}'")

# Download the cleaned file
#files.download(RBHcleaned_file_path)



import pandas as pd

# Load the three files
file_1NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _351 _48_ RB_Neu.asc'
file_2NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _352 _49_ RB_Neu.asc'
file_3NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _353 _50_ RB_Neu.asc'
file_4NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _355 _52_ RB_Neu.asc'
file_5NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _358 _54_ RB_Neu.asc'
file_6NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _359 _55_ RB_Neu.asc'
file_7NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _360 _56_ RB_Neu.asc'
file_8NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _362 _57_ RB_Neu.asc'
file_9NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _364 _59_ RB_Neu.asc'
file_10NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _365 _60_ RB_Neu.asc'
file_11NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _366 _61_ RB_Neu.asc'
file_12NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _367 _62_ RB_Neu.asc'
file_13NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _370 _65_ RB_Neu.asc'
file_14NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _372 _66_ RB_Neu.asc'
file_15NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _373 _67_ RB_Neu.asc'
file_16NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _374 _68_ RB_Neu.asc'
file_17NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _376 _70_ RB_Neu.asc'
file_18NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _379 _73_ RB_Neu.asc'
file_19NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _380 _74_ RB_Neu.asc'
file_20NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _382 _75_ RB_Neu.asc'
file_21NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _383 _76_ RB_Neu.asc'
file_22NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _384 _77_ RB_Neu.asc'
file_23NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _387 _80_ RB_Neu.asc'
file_24NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _388 _81_ RB_Neu.asc'
file_25NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _389 _82_ RB_Neu.asc'
file_26NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _390 _83_ RB_Neu.asc'
file_27NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _392 _85_ RB_Neu.asc'
file_28NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _393 _86_ RB_Neu.asc'
file_29NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _394 _87_ RB_Neu.asc'
file_30NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _395 _88_ RB_Neu.asc'
file_31NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _396 _89_ RB_Neu.asc'
file_32NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _398 _91_ RB_Neu.asc'
file_33NRB_path = '/content/drive/MyDrive/EMGDataset/R NEUROPATHY/EMG _399 _92_ RB_Neu.asc'




# Read the files into dataframes
df_1NRB = pd.read_csv(file_1NRB_path, delimiter='\t', header=None)
df_2NRB = pd.read_csv(file_2NRB_path, delimiter='\t', header=None)
df_3NRB = pd.read_csv(file_3NRB_path, delimiter='\t', header=None)
df_4NRB = pd.read_csv(file_4NRB_path, delimiter='\t', header=None)
df_5NRB = pd.read_csv(file_5NRB_path, delimiter='\t', header=None)
df_6NRB = pd.read_csv(file_6NRB_path, delimiter='\t', header=None)
df_7NRB = pd.read_csv(file_7NRB_path, delimiter='\t', header=None)
df_8NRB = pd.read_csv(file_8NRB_path, delimiter='\t', header=None)
df_9NRB = pd.read_csv(file_9NRB_path, delimiter='\t', header=None)
df_10NRB = pd.read_csv(file_10NRB_path, delimiter='\t', header=None)
df_11NRB = pd.read_csv(file_11NRB_path, delimiter='\t', header=None)
df_12NRB = pd.read_csv(file_12NRB_path, delimiter='\t', header=None)
df_13NRB = pd.read_csv(file_13NRB_path, delimiter='\t', header=None)
df_14NRB = pd.read_csv(file_14NRB_path, delimiter='\t', header=None)
df_15NRB = pd.read_csv(file_15NRB_path, delimiter='\t', header=None)
df_16NRB = pd.read_csv(file_16NRB_path, delimiter='\t', header=None)
df_17NRB = pd.read_csv(file_17NRB_path, delimiter='\t', header=None)
df_18NRB = pd.read_csv(file_18NRB_path, delimiter='\t', header=None)
df_19NRB = pd.read_csv(file_19NRB_path, delimiter='\t', header=None)
df_20NRB = pd.read_csv(file_20NRB_path, delimiter='\t', header=None)
df_21NRB = pd.read_csv(file_21NRB_path, delimiter='\t', header=None)
df_22NRB = pd.read_csv(file_22NRB_path, delimiter='\t', header=None)
df_23NRB = pd.read_csv(file_23NRB_path, delimiter='\t', header=None)
df_24NRB = pd.read_csv(file_24NRB_path, delimiter='\t', header=None)
df_25NRB = pd.read_csv(file_25NRB_path, delimiter='\t', header=None)
df_26NRB = pd.read_csv(file_26NRB_path, delimiter='\t', header=None)
df_27NRB = pd.read_csv(file_27NRB_path, delimiter='\t', header=None)
df_28NRB = pd.read_csv(file_28NRB_path, delimiter='\t', header=None)
df_29NRB = pd.read_csv(file_29NRB_path, delimiter='\t', header=None)
df_30NRB = pd.read_csv(file_30NRB_path, delimiter='\t', header=None)
df_31NRB = pd.read_csv(file_31NRB_path, delimiter='\t', header=None)
df_32NRB = pd.read_csv(file_32NRB_path, delimiter='\t', header=None)
df_33NRB = pd.read_csv(file_33NRB_path, delimiter='\t', header=None)






# Concatenate the dataframes row-wise
concatenated_NRBH = pd.concat([df_1NRB, df_2NRB, df_3NRB, df_4NRB, df_5NRB, df_6NRB, df_7NRB, df_8NRB
                             , df_9NRB, df_10NRB, df_11NRB, df_12NRB, df_13NRB, df_14NRB, df_15NRB, df_16NRB
                             , df_17NRB, df_18NRB, df_19NRB, df_20NRB, df_21NRB, df_22NRB, df_23NRB, df_24NRB
                             , df_25NRB, df_26NRB, df_27NRB, df_28NRB, df_29NRB, df_30NRB, df_31NRB, df_32NRB
                             , df_33NRB], axis=0, ignore_index=True)

# Check the datatypes
print(concatenated_NRBH.dtypes)

# Display the first few rows
concatenated_NRBH

import re

def read_asc_file(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()
        data = []
        for line in lines:
            # Use a regular expression to find all float numbers in the line
            numbers_str = re.findall(r'-?\d+\.\d+', line)
            # Convert these number strings to floats
            clean_numbers_float = [float(num) for num in numbers_str]
            data.append(clean_numbers_float)
    return data
# Read the files
data_1NRB = read_asc_file(file_1NRB_path)
data_2NRB = read_asc_file(file_2NRB_path)
data_3NRB = read_asc_file(file_3NRB_path)

data_4NRB= read_asc_file(file_4NRB_path)
data_5NRB= read_asc_file(file_5NRB_path)
data_6NRB= read_asc_file(file_6NRB_path)
data_7NRB= read_asc_file(file_7NRB_path)
data_8NRB= read_asc_file(file_8NRB_path)
data_9NRB= read_asc_file(file_9NRB_path)
data_10NRB= read_asc_file(file_10NRB_path)
data_11NRB= read_asc_file(file_11NRB_path)
data_12NRB= read_asc_file(file_12NRB_path)
data_13NRB= read_asc_file(file_13NRB_path)
data_14NRB= read_asc_file(file_14NRB_path)
data_15NRB= read_asc_file(file_15NRB_path)
data_16NRB= read_asc_file(file_16NRB_path)
data_17NRB= read_asc_file(file_17NRB_path)
data_18NRB= read_asc_file(file_18NRB_path)
data_19NRB= read_asc_file(file_19NRB_path)
data_20NRB= read_asc_file(file_20NRB_path)
data_21NRB= read_asc_file(file_21NRB_path)
data_22NRB= read_asc_file(file_22NRB_path)
data_23NRB= read_asc_file(file_23NRB_path)
data_24NRB= read_asc_file(file_24NRB_path)
data_25NRB= read_asc_file(file_25NRB_path)
data_26NRB= read_asc_file(file_26NRB_path)
data_27NRB= read_asc_file(file_27NRB_path)
data_28NRB= read_asc_file(file_28NRB_path)
data_29NRB= read_asc_file(file_29NRB_path)
data_30NRB= read_asc_file(file_30NRB_path)
data_31NRB= read_asc_file(file_31NRB_path)
data_32NRB= read_asc_file(file_32NRB_path)
data_33NRB= read_asc_file(file_33NRB_path)

# Concatenate the data
all_data1 = (data_1NRB + data_2NRB + data_3NRB + data_4NRB + data_5NRB + data_6NRB + data_7NRB + data_8NRB + data_9NRB + data_10NRB + data_11NRB + data_12NRB + data_13NRB + data_14NRB +
              data_15NRB + data_16NRB + data_17NRB + data_18NRB + data_19NRB + data_20NRB + data_21NRB + data_22NRB + data_23NRB + data_24NRB + data_25NRB + data_26NRB +
            data_27NRB + data_28NRB + data_29NRB + data_30NRB + data_31NRB + data_32NRB + data_33NRB

            )

# Convert to pandas DataFrame
df_NRB = pd.DataFrame(all_data1)

df_NRB

import pandas as pd
import numpy as np
# Save the cleaned data to a CSV file in the /content directory
local_file_path = '/content/df_NRB.csv'
df_NRB.to_csv(local_file_path, index=False)

print(f"Data cleaned and saved to '{local_file_path}'")

# Download the file
from google.colab import files
#files.download(local_file_path)

import pandas as pd
import numpy as np
from google.colab import files

# Load the dataset directly from the provided file path
file_path = '/content/df_NRB.csv'
data = pd.read_csv(file_path)

# Display the first few rows to understand its structure
print("Original Data:")
print(data.head())

# Step 1: Calculate the mean of the dataset
mean_value = data.mean().mean()

# Step 2: Replace NaNs with the mean value
NRBdata_cleaned = data.fillna(mean_value)

# Step 3: Replace unwanted values (e.g., negative values) with the mean value
# Assuming negative values are unwanted
NRBdata_cleaned[NRBdata_cleaned < 0] = mean_value

# Display the cleaned data
print("Cleaned Data:")
print(NRBdata_cleaned.head())

# Save the cleaned data to a new CSV file in the /content directory
NRBcleaned_file_path = '/content/NRBcleaned_data.csv'
NRBdata_cleaned.to_csv(NRBcleaned_file_path, index=False)

print(f"Cleaned data saved to '{NRBcleaned_file_path}'")

# Download the cleaned file
#files.download(NRBcleaned_file_path)

import pandas as pd

# Load the three files
file_1MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _301 _50_ RB_Myo.asc'
file_2MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _302 _51_ RB_Myo.asc'
file_3MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _303 _52_ RB_Myo.asc'
file_4MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _305 _54_ RB_Myo.asc'
file_5MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _306 _55_ RB_Myo.asc'

file_7MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _307 _56_ RB_Myo.asc'
file_8MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _308 _57_ RB_Myo.asc'
file_9MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _310 _59_ RB_Myo.asc'
file_10MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _311 _60_ RB_Myo.asc'

file_12MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _313 _62_ RB_Myo.asc'
file_13MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _314 _63_ RB_Myo.asc'
file_14MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _315 _64_ RB_Myo.asc'
file_15MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _316 _65_ RB_Myo.asc'
file_16MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _318 _67_ RB_Myo.asc'
file_17MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _321 _70_ RB_Myo.asc'
file_18MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _325 _74_ RB_Myo.asc'
file_19MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _326 _75_ RB_Myo.asc'
file_20MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _327 _76_ RB_Myo.asc'
file_21MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _328 _77_ RB_Myo.asc'
file_22MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _331 _80_ RB_Myo.asc'
file_23MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _335 _84_ RB_Myo.asc'
file_24MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _337 _86_ RB_Myo.asc'
file_25MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _338 _87_ RB_Myo.asc'
file_26MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _340 _89_ RB_Myo.asc'
file_27MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _342 _90_ RB_Myo.asc'
file_28MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _343 _91_ RB_Myo.asc'
file_29MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _344 _92_ RB_Myo.asc'
file_30MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _345 _93_ RB_Myo.asc'
file_31MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _347 _95_ RB_Myo.asc'
file_32MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _348 _96_ RB_Myo.asc'
file_33MRB_path = '/content/drive/MyDrive/EMGDataset/R MYOPATHY/EMG _349 _97_ RB_Myo.asc'




# Read the files into dataframes
df_1MRB = pd.read_csv(file_1MRB_path, delimiter='\t', header=None)
df_2MRB = pd.read_csv(file_2MRB_path, delimiter='\t', header=None)
df_3MRB = pd.read_csv(file_3MRB_path, delimiter='\t', header=None)
df_4MRB = pd.read_csv(file_4MRB_path, delimiter='\t', header=None)
df_5MRB = pd.read_csv(file_5MRB_path, delimiter='\t', header=None)

df_7MRB = pd.read_csv(file_7MRB_path, delimiter='\t', header=None)
df_8MRB = pd.read_csv(file_8MRB_path, delimiter='\t', header=None)
df_9MRB = pd.read_csv(file_9MRB_path, delimiter='\t', header=None)
df_10MRB = pd.read_csv(file_10MRB_path, delimiter='\t', header=None)

df_12MRB = pd.read_csv(file_12MRB_path, delimiter='\t', header=None)
df_13MRB = pd.read_csv(file_13MRB_path, delimiter='\t', header=None)
df_14MRB = pd.read_csv(file_14MRB_path, delimiter='\t', header=None)
df_15MRB = pd.read_csv(file_15MRB_path, delimiter='\t', header=None)
df_16MRB = pd.read_csv(file_16MRB_path, delimiter='\t', header=None)
df_17MRB = pd.read_csv(file_17MRB_path, delimiter='\t', header=None)
df_18MRB = pd.read_csv(file_18MRB_path, delimiter='\t', header=None)
df_19MRB = pd.read_csv(file_19MRB_path, delimiter='\t', header=None)
df_20MRB = pd.read_csv(file_20MRB_path, delimiter='\t', header=None)
df_21MRB = pd.read_csv(file_21MRB_path, delimiter='\t', header=None)
df_22MRB = pd.read_csv(file_22MRB_path, delimiter='\t', header=None)
df_23MRB = pd.read_csv(file_23MRB_path, delimiter='\t', header=None)
df_24MRB = pd.read_csv(file_24MRB_path, delimiter='\t', header=None)
df_25MRB = pd.read_csv(file_25MRB_path, delimiter='\t', header=None)
df_26MRB = pd.read_csv(file_26MRB_path, delimiter='\t', header=None)
df_27MRB = pd.read_csv(file_27MRB_path, delimiter='\t', header=None)
df_28MRB = pd.read_csv(file_28MRB_path, delimiter='\t', header=None)
df_29MRB = pd.read_csv(file_29MRB_path, delimiter='\t', header=None)
df_30MRB = pd.read_csv(file_30MRB_path, delimiter='\t', header=None)
df_31MRB = pd.read_csv(file_31MRB_path, delimiter='\t', header=None)
df_32MRB = pd.read_csv(file_32MRB_path, delimiter='\t', header=None)
df_33MRB = pd.read_csv(file_33MRB_path, delimiter='\t', header=None)




# Concatenate the dataframes row-wise
concatenated_MRBH = pd.concat([df_1MRB, df_2MRB, df_3MRB, df_4MRB, df_5MRB,  df_7MRB, df_8MRB
                             , df_9MRB, df_10MRB, df_12MRB, df_13MRB, df_14MRB, df_15MRB, df_16MRB
                             , df_17MRB, df_18MRB, df_19MRB, df_20MRB, df_21MRB, df_22MRB, df_23MRB, df_24MRB
                             , df_25MRB, df_26MRB, df_27MRB, df_28MRB, df_29MRB, df_30MRB, df_31MRB, df_32MRB
                             , df_33MRB], axis=0, ignore_index=True)

# Check the datatypes
print(concatenated_MRBH.dtypes)

# Display the first few rows
concatenated_MRBH

import re

def read_asc_file(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()
        data = []
        for line in lines:
            # Use a regular expression to find all float numbers in the line
            numbers_str = re.findall(r'-?\d+\.\d+', line)
            # Convert these number strings to floats
            clean_numbers_float = [float(num) for num in numbers_str]
            data.append(clean_numbers_float)
    return data
# Read the files
data_1MRB = read_asc_file(file_1MRB_path)
data_2MRB = read_asc_file(file_2MRB_path)
data_3MRB = read_asc_file(file_3MRB_path)

data_4MRB= read_asc_file(file_4MRB_path)
data_5MRB= read_asc_file(file_5MRB_path)

data_7MRB= read_asc_file(file_7MRB_path)
data_8MRB= read_asc_file(file_8MRB_path)
data_9MRB= read_asc_file(file_9MRB_path)
data_10MRB= read_asc_file(file_10MRB_path)

data_12MRB= read_asc_file(file_12MRB_path)
data_13MRB= read_asc_file(file_13MRB_path)
data_14MRB= read_asc_file(file_14MRB_path)
data_15MRB= read_asc_file(file_15MRB_path)
data_16MRB= read_asc_file(file_16MRB_path)
data_17MRB= read_asc_file(file_17MRB_path)
data_18MRB= read_asc_file(file_18MRB_path)
data_19MRB= read_asc_file(file_19MRB_path)
data_20MRB= read_asc_file(file_20MRB_path)
data_21MRB= read_asc_file(file_21MRB_path)
data_22MRB= read_asc_file(file_22MRB_path)
data_23MRB= read_asc_file(file_23MRB_path)
data_24MRB= read_asc_file(file_24MRB_path)
data_25MRB= read_asc_file(file_25MRB_path)
data_26MRB= read_asc_file(file_26MRB_path)
data_27MRB= read_asc_file(file_27MRB_path)
data_28MRB= read_asc_file(file_28MRB_path)
data_29MRB= read_asc_file(file_29MRB_path)
data_30MRB= read_asc_file(file_30MRB_path)
data_31MRB= read_asc_file(file_31MRB_path)
data_32MRB= read_asc_file(file_32MRB_path)
data_33MRB= read_asc_file(file_33MRB_path)

# Concatenate the data
all_data2 = (data_1MRB + data_2MRB + data_3MRB + data_4MRB + data_5MRB +  data_7MRB + data_8MRB + data_9MRB + data_10MRB +  data_12MRB + data_13MRB + data_14MRB +
              data_15MRB + data_16MRB + data_17MRB + data_18MRB + data_19MRB + data_20MRB + data_21MRB + data_22MRB + data_23MRB + data_24MRB + data_25MRB + data_26MRB +
            data_27MRB + data_28MRB + data_29MRB + data_30MRB + data_31MRB + data_32MRB + data_33MRB

            )

# Convert to pandas DataFrame
df_MRB = pd.DataFrame(all_data2)

df_MRB

import pandas as pd
import numpy as np
# Save the cleaned data to a CSV file in the /content directory
local_file_path = '/content/df_MRB.csv'
df_MRB.to_csv(local_file_path, index=False)

print(f"Data  saved to '{local_file_path}'")

# Download the file
from google.colab import files
#files.download(local_file_path)

import pandas as pd
import numpy as np
from google.colab import files

# Load the dataset directly from the provided file path
file_path = '/content/df_MRB.csv'
data = pd.read_csv(file_path)

# Display the first few rows to understand its structure
print("Original Data:")
print(data.head())

# Step 1: Calculate the mean of the dataset
mean_value = data.mean().mean()

# Step 2: Replace NaNs with the mean value
MRBHdata_cleaned = data.fillna(mean_value)

# Step 3: Replace unwanted values (e.g., negative values) with the mean value
# Assuming negative values are unwanted
MRBHdata_cleaned[MRBHdata_cleaned < 0] = mean_value

# Display the cleaned data
print("Cleaned Data:")
print(MRBHdata_cleaned.head())

# Save the cleaned data to a new CSV file in the /content directory
MRBHcleaned_file_path = '/content/MRBHcleaned_data.csv'
MRBHdata_cleaned.to_csv(MRBHcleaned_file_path, index=False)

print(f"Cleaned data saved to '{MRBHcleaned_file_path}'")

# Download the cleaned file
#files.download(MRBHcleaned_file_path)

mean_value

RBHdata_cleaned

copy_RBHdata_cleaned = RBHdata_cleaned.transpose()

copy_RBHdata_cleaned

import pandas as pd
from sklearn.preprocessing import MinMaxScaler


# Check the shape of the data
print("Original data shape:", copy_RBHdata_cleaned.shape)

# Initialize the Min-Max scaler
scaler = MinMaxScaler()

# Fit and transform the data using Min-Max scaling
scaled_data = scaler.fit_transform(copy_RBHdata_cleaned)

# Convert the scaled data back to a DataFrame
scaled_df = pd.DataFrame(scaled_data, columns=copy_RBHdata_cleaned.columns)

# Save the scaled data to a new CSV file
scaled_copy_RBHdata_cleaned = '/content/RBHcleaned_data_scaled.csv'
scaled_df.to_csv(scaled_copy_RBHdata_cleaned, index=False)

print(f"Scaled data saved to {scaled_copy_RBHdata_cleaned}")
#files.download(scaled_copy_RBHdata_cleaned)

RBH = pd.read_csv(scaled_copy_RBHdata_cleaned)
RBH

!pip install PyWavelets

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pywt
from scipy.signal import welch

# Load

# Assuming the data is in a single column
signal = RBH .iloc[:, 0]

# Perform Continuous Wavelet Transform (CWT)
widths = np.arange(1, 128)
cwt_matrix, freqs = pywt.cwt(signal, widths, 'morl')

# Plot CWT
plt.figure(figsize=(10, 6))
plt.imshow(np.abs(cwt_matrix), extent=[0, len(signal), 1, 128], cmap='PRGn', aspect='auto',
           vmax=abs(cwt_matrix).max(), vmin=-abs(cwt_matrix).max())
plt.colorbar(label='Magnitude')
plt.ylabel('Scale')
plt.xlabel('Sample')
plt.title('Continuous Wavelet Transform (CWT)')
plt.show()

# Compute Power Spectral Density (PSD)
frequencies, psd = welch(signal, fs=32768, nperseg=1024)

# Plot PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density (PSD)')
plt.title('Power Spectral Density (PSD)')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import welch

# Sample data (replace this with your actual data)


# Extract the signal (assuming the signal is in the 'signal' column)
signal = RBH.iloc[:, 0]
# Define the sampling frequency (Fs) and window size
Fs = 32768  # Example: 500 Hz, adjust according to your data
window_size = 1 # Window size in seconds
nperseg = int(Fs * window_size)  # Number of samples per segment
noverlap = nperseg // 2  # 50% overlap

# Calculate the Power Spectral Density (PSD) using Welch's method
frequencies, psd = welch(signal, fs=Fs, nperseg=nperseg, noverlap=noverlap)

# Plot the PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.title('Power Spectral Density (PSD)')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power/Frequency (dB/Hz)')
plt.grid()
plt.show()

import pandas as pd
import numpy as np

# Load the dataset


import pandas as pd
import numpy as np

# Load the dataset
file_path = '/content/RBHcleaned_data_scaled.csv'  # update this with your file path
data = pd.read_csv(file_path)

# Parameters
window_size = 1  # Window size
overlap = 0.5  # Overlap
sampling_rate = 500 # Sampling rate

# Function to create segments
def create_segments(data, window_size, overlap, sampling_rate):
    segments = []
    num_segments = int((len(data) - window_size*sampling_rate) / (window_size*sampling_rate - overlap*sampling_rate)) + 1
    for i in range(num_segments):
        start = i * (window_size - overlap)
        end = start + window_size
        segment = data[int(start*sampling_rate):int(end*sampling_rate)]
        segments.append(segment)
    return np.array(segments)

# Apply segmentation to each column
segmented_dataRBH = {}

for column in data.columns:
    human_data = data[column].values
    segments = create_segments(human_data, window_size, overlap, sampling_rate)
    print(segments.shape)
    segmented_dataRBH[column] = segments
    print(f'Segment {column} Completed')


# Example: access segments for the first human (first column)
first_human_segmentsRBH = segmented_dataRBH[data.columns[0]]


# Print some information about the segments
print(f"Number of segments for first human: {len(first_human_segmentsRBH)}")
print(f"First segment: {first_human_segmentsRBH[0]}")

first_human_segmentsRBH[0].shape



import pandas as pd
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt



# Assume the signal data is in a column named 'signal' (update as necessary)
signal = first_human_segmentsRBH[0]

# Parameters
fs = 500 # Sampling frequency
window_size = fs * 1  # Window size of 1 second
overlap = 0.5  # Overlap of 0.5
nperseg = window_size
noverlap = int(nperseg * overlap)

# Compute the PSD using Welch's method
frequencies, psd = welch(signal, fs, nperseg=nperseg, noverlap=noverlap)

# Plot the PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.title('Power Spectral Density (PSD)')
plt.xlabel('Frequency (Hz)')
plt.ylabel('PSD (V^2/Hz)')
plt.grid()
plt.show()

# Plot the histogram of the PSD
plt.figure(figsize=(10, 6))
plt.hist(psd, bins=50, log=True)  # Using a log scale for better visibility
plt.title('Histogram of Power Spectral Density (PSD) Values')
plt.xlabel('PSD (V^2/Hz)')
plt.ylabel('Frequency of occurrence (number of times each power value appears within the PSD)')
plt.grid()
plt.show()

rbh = np.concatenate(np.array(list(segmented_dataRBH.values())),0)

rbh.shape

import numpy as np
from scipy.signal import welch

# Assuming data is your 2D array with shape (279, 32768)
 # Example data, replace with actual data

# Initialize an empty list to store the PSD results
psd_list = []

# Loop through each internal array and compute the PSD
for i in range(rbh.shape[0]):
    f, Pxx = welch(rbh[i], fs=500, nperseg=256)
    psd_list.append(Pxx)

# Convert the list to a numpy array for easier handling
psd_arrayrbh = np.array(psd_list)

# psd_array now contains the PSD values for each internal array
print(psd_arrayrbh)

psd_arrayrbh.shape

import numpy as np

# Define the 2D array using NumPy


# Create a new column filled with ones
new_column = np.ones((psd_arrayrbh.shape[0], 1))

# Append the new column to the original psd_arrayrbh
updated_psd_arrayrbh = np.hstack((psd_arrayrbh, new_column))

# Print the updated psd_arrayrbh
print(updated_psd_arrayrbh)

updated_psd_arrayrbh

psd_arrayrbh.shape

import csv

# Example 2D array


# Specify the file name
csv_file = 'psdarrayrbh.csv'

# Open the file in write mode
with open(csv_file, 'w', newline='') as file:
    writer = csv.writer(file)
    for row in psd_arrayrbh:
        writer.writerow(row)

print(f'CSV file {csv_file} has been created successfully.')

psdrbh = pd.read_csv(csv_file)
psdrbh

psdrbh.shape

copy_NRBdata_cleaned = NRBdata_cleaned.transpose()
copy_NRBdata_cleaned

import pandas as pd
from sklearn.preprocessing import MinMaxScaler


# Check the shape of the data
print("Original data shape:", copy_NRBdata_cleaned.shape)

# Initialize the Min-Max scaler
scaler = MinMaxScaler()

# Fit and transform the data using Min-Max scaling
scaled_data = scaler.fit_transform(copy_NRBdata_cleaned)

# Convert the scaled data back to a DataFrame
scaled_df = pd.DataFrame(scaled_data, columns=copy_NRBdata_cleaned.columns)

# Save the scaled data to a new CSV file
scaled_copy_NRBdata_cleaned = '/content/NRBcleaned_data_scaled.csv'
scaled_df.to_csv(scaled_copy_NRBdata_cleaned, index=False)

print(f"Scaled data saved to {scaled_copy_NRBdata_cleaned}")

#files.download(scaled_copy_NRBdata_cleaned)

NRB = pd.read_csv(scaled_copy_NRBdata_cleaned)
NRB

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pywt
from scipy.signal import welch

# Assuming the data is in a single column
signal = NRB.iloc[:, 0]

# Perform Continuous Wavelet Transform (CWT)
widths = np.arange(1, 128)
cwt_matrix, freqs = pywt.cwt(signal, widths, 'morl')

# Plot CWT
plt.figure(figsize=(10, 6))
plt.imshow(np.abs(cwt_matrix), extent=[0, len(signal), 1, 128], cmap='PRGn', aspect='auto',
           vmax=abs(cwt_matrix).max(), vmin=-abs(cwt_matrix).max())
plt.colorbar(label='Magnitude')
plt.ylabel('Scale')
plt.xlabel('Sample')
plt.title('Continuous Wavelet Transform (CWT)')
plt.show()

# Compute Power Spectral Density (PSD)
frequencies, psd = welch(signal, fs=32768, nperseg=1024)

# Plot PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density (PSD)')
plt.title('Power Spectral Density (PSD)')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import welch

# Sample data (replace this with your actual data)


# Extract the signal (assuming the signal is in the 'signal' column)
signal = NRB.iloc[:, 0]
# Define the sampling frequency (Fs) and window size
Fs = 32768  # Example: 500 Hz, adjust according to your data
window_size = 1 # Window size in seconds
nperseg = int(Fs * window_size)  # Number of samples per segment
noverlap = nperseg // 2  # 50% overlap

# Calculate the Power Spectral Density (PSD) using Welch's method
frequencies, psd = welch(signal, fs=Fs, nperseg=nperseg, noverlap=noverlap)

# Plot the PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.title('Power Spectral Density (PSD)')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power/Frequency (dB/Hz)')
plt.grid()
plt.show()

import pandas as pd
import numpy as np

# Load the dataset


import pandas as pd
import numpy as np

# Load the dataset
file_path = '/content/NRBcleaned_data_scaled.csv'  # update this with your file path
data = pd.read_csv(file_path)

# Parameters
window_size = 1  # Window size
overlap = 0.5  # Overlap
sampling_rate = 500 # Sampling rate

# Function to create segments
def create_segments(data, window_size, overlap, sampling_rate):
    segments = []
    num_segments = int((len(data) - window_size*sampling_rate) / (window_size*sampling_rate - overlap*sampling_rate)) + 1
    for i in range(num_segments):
        start = i * (window_size - overlap)
        end = start + window_size
        segment = data[int(start*sampling_rate):int(end*sampling_rate)]
        segments.append(segment)
    return np.array(segments)

# Apply segmentation to each column
segmented_dataNRB = {}

for column in data.columns:
    human_data = data[column].values
    segments = create_segments(human_data, window_size, overlap, sampling_rate)
    print(segments.shape)
    segmented_dataNRB[column] = segments
    print(f'Segment {column} Completed')

# Example: access segments for the first human (first column)
first_human_segmentsNRB = segmented_dataNRB[data.columns[0]]

# Print some information about the segments
print(f"Number of segments for first human: {len(first_human_segmentsNRB)}")
print(f"First segment: {first_human_segmentsNRB[0]}")

segmented_dataNRB

first_human_segmentsNRB[0]

# @title Default title text
first_human_segmentsNRB[0].shape

import numpy as np
import csv




# Convert the NumPy array to a list of lists
array_as_list = first_human_segmentsNRB.tolist()

# Specify the filename
csv_filename = 'outputNRB.csv'

# Open the CSV file in write mode
with open(csv_filename, mode='w', newline='') as file:
    writer = csv.writer(file)

    # Write each row of the list to the CSV file
    for row in array_as_list:
        # Ensure the row is iterable
        if isinstance(row, (list, tuple)):
            writer.writerow(row)
        else:
            # Handle non-iterable elements (if any)
            writer.writerow([row])

print(f"The array has been successfully written to {csv_filename}")
segment_NRB= pd.read_csv(csv_filename)
segment_NRB
#files.download(csv_filename)

import pandas as pd
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt

# Load the CSV file
file_path = 'outputNRB.csv'
data = pd.read_csv(file_path)

# Initialize an empty list to hold the PSD data
psd_data = []

# Compute the PSD for each segment (each row in the CSV file)
for i, segment in enumerate(data.values):
    freqs, psd = welch(segment)
    psd_data.append(psd)

    # Plotting each segment's PSD
    plt.plot(freqs, psd, label=f'Segment {i+1}')

# Convert the PSD data to a DataFrame and save it to a new CSV file
psd_df = pd.DataFrame(psd_data)
psd_df.to_csv('psd_output.csv', index=False)

# Plotting configuration
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density (PSD)')
plt.title('PSD of Each Segment')
plt.legend()
plt.show()

psd_df

nrb = np.concatenate(np.array(list(segmented_dataNRB.values())),0)
nrb

nrb.shape



import numpy as np
from scipy.signal import welch

# Assuming data is your 2D array with shape (279, 32768)
  # Example data, replace with actual data

# Initialize an empty list to store the PSD results
psd_list = []

# Loop through each internal array and compute the PSD
for i in range(nrb.shape[0]):
    f, Pxx = welch(nrb[i], fs=500, nperseg=256)
    psd_list.append(Pxx)

# Convert the list to a numpy array for easier handling
psd_arraynrb = np.array(psd_list)

# psd_array now contains the PSD values for each internal array
print(psd_arraynrb)

import numpy as np



# Create a new column filled with twos
new_column = np.full((psd_arraynrb.shape[0], 1), 2)

# Append the new column to the original psd_arraynrb
updated_psd_arraynrb = np.hstack((psd_arraynrb, new_column))

# Print the updated psd_arraynrb
print(updated_psd_arraynrb)

updated_psd_arraynrb.shape

import pandas as pd
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt



# Assume the signal data is in a column named 'signal' (update as necessary)
signal = first_human_segmentsNRB[0]

# Parameters
fs = 500  # Sampling frequency
window_size = fs * 1  # Window size of 1 second
overlap = 0.5  # Overlap of 0.5
nperseg = window_size
noverlap = int(nperseg * overlap)

# Compute the PSD using Welch's method
frequencies, psd = welch(signal, fs, nperseg=nperseg, noverlap=noverlap)

# Plot the PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.title('Power Spectral Density (PSD)')
plt.xlabel('Frequency (Hz)')
plt.ylabel('PSD (V^2/Hz)')
plt.grid()
plt.show()

# Plot the histogram of the PSD
plt.figure(figsize=(10, 6))
plt.hist(psd, bins=50, log=True)  # Using a log scale for better visibility
plt.title('Histogram of Power Spectral Density (PSD) Values')
plt.xlabel('PSD (V^2/Hz)')
plt.ylabel('Frequency of occurrence (number of times each power value appears within the PSD)')
plt.grid()
plt.show()

# Save PSD values to a CSV file
psd_data = pd.DataFrame({'Frequency (Hz)': frequencies, 'PSD (V^2/Hz)': psd})
psd_data.to_csv('psd_values.csv', index=False)

print("PSD values saved to psd_values.csv")

psd=psd_data
psd

psd=psd_data
psd

MRBHdata_cleaned.shape
data = MRBHdata_cleaned.values
data = data*1e-3

plt.plot(data[0])

copy_MRBHdata_cleaned = MRBHdata_cleaned.transpose()
copy_MRBHdata_cleaned
import pandas as pd
from sklearn.preprocessing import MinMaxScaler


# Check the shape of the data
print("Original data shape:", copy_MRBHdata_cleaned.shape)

# Initialize the Min-Max scaler
scaler = MinMaxScaler()

# Fit and transform the data using Min-Max scaling
scaled_data = scaler.fit_transform(copy_MRBHdata_cleaned)

# Convert the scaled data back to a DataFrame
scaled_df = pd.DataFrame(scaled_data, columns=copy_MRBHdata_cleaned.columns)

# Save the scaled data to a new CSV file
scaled_copy_MRBHdata_cleaned = '/content/MRBHcleaned_data_scaled.csv'
scaled_df.to_csv(scaled_copy_MRBHdata_cleaned, index=False)

print(f"Scaled data saved to {scaled_copy_MRBHdata_cleaned}")

#files.download(scaled_copy_MRBHdata_cleaned)


MRBH = pd.read_csv(scaled_copy_MRBHdata_cleaned)
MRBH

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pywt
from scipy.signal import welch

# Load

# Assuming the data is in a single column
signal = MRBH.iloc[:, 0]

# Perform Continuous Wavelet Transform (CWT)
widths = np.arange(1, 128)
cwt_matrix, freqs = pywt.cwt(signal, widths, 'morl')

# Plot CWT
plt.figure(figsize=(10, 6))
plt.imshow(np.abs(cwt_matrix), extent=[0, len(signal), 1, 128], cmap='PRGn', aspect='auto',
           vmax=abs(cwt_matrix).max(), vmin=-abs(cwt_matrix).max())
plt.colorbar(label='Magnitude')
plt.ylabel('Scale')
plt.xlabel('Sample')
plt.title('Continuous Wavelet Transform (CWT)')
plt.show()

# Compute Power Spectral Density (PSD)
frequencies, psd = welch(signal, fs=32768, nperseg=1024)

# Plot PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power Spectral Density (PSD)')
plt.title('Power Spectral Density (PSD)')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import welch

# Sample data (replace this with your actual data)


# Extract the signal (assuming the signal is in the 'signal' column)
signal = MRBH.iloc[:, 0]
# Define the sampling frequency (Fs) and window size
Fs = 32768  # Example: 500 Hz, adjust according to your data
window_size = 1 # Window size in seconds
nperseg = int(Fs * window_size)  # Number of samples per segment
noverlap = nperseg // 2  # 50% overlap

# Calculate the Power Spectral Density (PSD) using Welch's method
frequencies, psd = welch(signal, fs=Fs, nperseg=nperseg, noverlap=noverlap)

# Plot the PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.title('Power Spectral Density (PSD)')
plt.xlabel('Frequency (Hz)')
plt.ylabel('Power/Frequency (dB/Hz)')
plt.grid()
plt.show()

import pandas as pd
import numpy as np

# Load the dataset


import pandas as pd
import numpy as np

# Load the dataset
file_path = '/content/MRBHcleaned_data_scaled.csv'  # update this with your file path
data = pd.read_csv(file_path)

# Parameters
window_size = 1  # Window size
overlap = 0.5  # Overlap
sampling_rate = 500 # Sampling rate

# Function to create segments
def create_segments(data, window_size, overlap, sampling_rate):
    segments = []
    num_segments = int((len(data) - window_size*sampling_rate) / (window_size*sampling_rate - overlap*sampling_rate)) + 1
    for i in range(num_segments):
        start = i * (window_size - overlap)
        end = start + window_size
        segment = data[int(start*sampling_rate):int(end*sampling_rate)]
        segments.append(segment)
    return np.array(segments)

# Apply segmentation to each column
segmented_dataMRB = {}

for column in data.columns:
    human_data = data[column].values
    segments = create_segments(human_data, window_size, overlap, sampling_rate)
    print(segments.shape)
    segmented_dataMRB[column] = segments
    print(f'Segment {column} Completed')

# Example: access segments for the first human (first column)


segments_MRB = {}
for i in range(31):
  segments_MRB[str(i)] = segmented_dataMRB[data.columns[i]]



np.array(list(segments_MRB.values())).shape

np.concatenate(np.array(list(segments_MRB.values())),0).shape

mrb = np.concatenate(np.array(list(segments_MRB.values())),0)
mrb

mrb[1]

import numpy as np
import csv




# Convert the NumPy array to a list of lists
array_as_list = mrb[0].tolist()

# Specify the filename
csv_filename = 'outputMRB.csv'

# Open the CSV file in write mode
with open(csv_filename, mode='w', newline='') as file:
    writer = csv.writer(file)

    # Write each row of the list to the CSV file
    for row in array_as_list:
        # Ensure the row is iterable
        if isinstance(row, (list, tuple)):
            writer.writerow(row)
        else:
            # Handle non-iterable elements (if any)
            writer.writerow([row])

print(f"The array has been successfully written to {csv_filename}")
segment_firstMRB= pd.read_csv(csv_filename)
segment_firstMRB

import pandas as pd
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt



# Assume the signal data is in a column named 'signal' (update as necessary)
signal = mrb[1]

# Parameters
fs = 500 # Sampling frequency
window_size = fs * 1  # Window size of 1 second
overlap = 0.5  # Overlap of 0.5
nperseg = window_size
noverlap = int(nperseg * overlap)

# Compute the PSD using Welch's method
frequencies, psd = welch(signal, fs, nperseg=nperseg, noverlap=noverlap)

# Plot the PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.title('Power Spectral Density (PSD)')
plt.xlabel('Frequency (Hz)')
plt.ylabel('PSD (V^2/Hz)')
plt.grid()
plt.show()

# Plot the histogram of the PSD
plt.figure(figsize=(10, 6))
plt.hist(psd, bins=50, log=True)  # Using a log scale for better visibility
plt.title('Histogram of Power Spectral Density (PSD) Values')
plt.xlabel('PSD (V^2/Hz)')
plt.ylabel('Frequency of occurrence (number of times each power value appears within the PSD)')
plt.grid()
plt.show()

# @title Default title text
mrb

import numpy as np
from scipy.signal import welch

# Assuming data is your 2D array with shape (279, 32768)
mrb  # Example data, replace with actual data

# Initialize an empty list to store the PSD results
psd_list = []

# Loop through each internal array and compute the PSD
for i in range(mrb.shape[0]):
    f, Pxx = welch(mrb[i], fs=500, nperseg=256)
    psd_list.append(Pxx)

# Convert the list to a numpy array for easier handling
psd_arraymrb = np.array(psd_list)

# psd_array now contains the PSD values for each internal array
print(psd_arraymrb)

psd_arraymrb.shape

psd_arraymrb.shape

import numpy as np



# Create a new column filled with threes
new_column = np.full((psd_arraymrb.shape[0], 1), 3)

# Append the new column to the original
updated_psd_arraymrb = np.hstack((psd_arraymrb, new_column))

# Print the updated
print(updated_psd_arraymrb)

updated_psd_arraymrb.shape

import csv

# Example 2D array


# Specify the file name
csv_file = 'psdarraymrb.csv'

# Open the file in write mode
with open(csv_file, 'w', newline='') as file:
    writer = csv.writer(file)
    for row in psd_arraymrb:
        writer.writerow(row)

print(f'CSV file {csv_file} has been created successfully.')

psdm = pd.read_csv(csv_file)
psdm

psdm.shape

import pandas as pd
import numpy as np
from scipy.signal import welch
import matplotlib.pyplot as plt



# Assume the signal data is in a column named 'signal' (update as necessary)
signal = mrb[1]
# Parameters
fs = 500  # Sampling frequency
window_size = fs * 1  # Window size of 1 second
overlap = 0.5  # Overlap of 0.5
nperseg = window_size
noverlap = int(nperseg * overlap)

# Compute the PSD using Welch's method
frequencies, psd = welch(signal, fs, nperseg=nperseg, noverlap=noverlap)

# Plot the PSD
plt.figure(figsize=(10, 6))
plt.semilogy(frequencies, psd)
plt.title('Power Spectral Density (PSD)')
plt.xlabel('Frequency (Hz)')
plt.ylabel('PSD (V^2/Hz)')
plt.grid()
plt.show()

# Plot the histogram of the PSD
plt.figure(figsize=(10, 6))
plt.hist(psd, bins=50, log=True)  # Using a log scale for better visibility
plt.title('Histogram of Power Spectral Density (PSD) Values')
plt.xlabel('PSD (V^2/Hz)')
plt.ylabel('Frequency of occurrence (number of times each power value appears within the PSD)')
plt.grid()
plt.show()

'''import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Load the data from the CSV files
data_nrb = pd.read_csv('/content/outputNRB.csv')
copy_data_nrb = data_nrb.transpose()

data_rbh = pd.read_csv('/content/outputRBH.csv')
copy_data_rbh = data_rbh.transpose()
data_mrbh = pd.read_csv('/content/outputMRB.csv')
copy_data_mrbh = data_mrbh.transpose()

# Add a source column to each dataset
copy_data_nrb['source'] = 'NRB'
copy_data_rbh['source'] = 'RBH'
copy_data_mrbh['source'] = 'MRBH'

# Combine the data into one Da
data_combined = pd.concat([copy_data_nrb, copy_data_rbh, copy_data_mrbh])

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42)
data_tsne = tsne.fit_transform(data_combined.drop('source', axis=1))

# Plot the t-SNE results with color coding
plt.figure(figsize=(10, 8))
categories = data_combined['source'].unique()
colors = plt.cm.rainbow(np.linspace(0, 1, len(categories)))

for category, color in zip(categories, colors):
    indices = data_combined['source'] == category
    plt.scatter(data_tsne[indices, 0], data_tsne[indices, 1], label=category, color=color)

plt.title('t-SNE Plot with Color-Coded Categories')
plt.xlabel('t-SNE Feature 1')
plt.ylabel('t-SNE Feature 2')
plt.legend()
plt.show()
'''

import numpy as np

# Concatenate the three 2D arrays row-wise
result_matrix = np.vstack((updated_psd_arrayrbh, updated_psd_arraynrb , updated_psd_arraymrb ))

# Print the resulting matrix
print(result_matrix)

updated_psd_arrayrbh.shape

updated_psd_arraynrb.shape

updated_psd_arraymrb.shape

from google.colab import drive
drive.mount('/content/drive')

result_matrix.shape

type(result_matrix[0])

!pip install scipy

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Separate features and target
X = result_matrix[:, :-1]
y = result_matrix[:, -1]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

# Initialize the Random Forest model
model = RandomForestClassifier(n_estimators=250)

# Train the model
model.fit(X_train , y_train)

# Make predictions
y_pred = model.predict(X_train)

y_pred

# Evaluate the model
accuracy = accuracy_score(y_train, y_pred)
report = classification_report(y_train, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

from google.colab import files
from google.colab import files

import pickle

# Save the model (same as before)
with open('modelrandomforest.pkl', 'wb') as file:
    pickle.dump(model, file)

# Download the file
files.download('modelrandomforest.pkl')

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Assuming X and y are already defined
# X, y = ...

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Define a smaller parameter grid for Grid Search
param_grid = {
    'n_estimators': [300, 350],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt']
}

# Perform Grid Search with Cross-Validation with reduced jobs
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=1, scoring='accuracy')

# Use joblib's parallel_backend to set inner_max_num_threads
with joblib.parallel_backend('loky', inner_max_num_threads=1):
    grid_search.fit(X_train, y_train)

# Get the best parameters from Grid Search
best_params = grid_search.best_params_

# Train the Random Forest model with the best parameters
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=['healthy', 'neuropathy', 'myopathy'])

print(f'Best Parameters: {best_params}')
print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(report)

from sklearn.linear_model import LogisticRegression

# Initialize the model
Lf = LogisticRegression()

# Train the Lf
Lf.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report

# Make predictions
y_pred = Lf.predict(X_test)

# Evaluate the Lf
accuracy = accuracy_score( y_pred, y_test)
report = classification_report( y_pred, y_test)

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

# Make predictions
y_pred = Lf.predict(X_train)

# Evaluate the model
accuracy = accuracy_score( y_pred, y_train)
report = classification_report( y_pred, y_train)

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{report}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# Create and train the SVM model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=['healthy', 'neuropathy', 'myopathy'])

print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(report)


print(y_pred)
y_test

# Make predictions on the test set
y_pred = svm_model.predict(X_train)

# Evaluate the model
accuracy = accuracy_score(y_train, y_pred)
report = classification_report(y_train, y_pred, target_names=['healthy', 'neuropathy', 'myopathy'])

print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(report)


print(y_pred)
y_train

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score



# Step 3: Train the model
knn = KNeighborsClassifier(n_neighbors=3)  # You can change the number of neighbors (k) as needed
knn.fit(X_train, y_train)

# Step 4: Evaluate the model
y_pred = knn.predict(X_train)
accuracy = accuracy_score(y_train, y_pred)
report = classification_report(y_train, y_pred)

print(f'Accuracy: {accuracy * 100:.2f}%')
print(report)

# Step 4: Evaluate the model
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy * 100:.2f}%')
print(report)

from sklearn.metrics import confusion_matrix

# Predict on test data
y_test_pred =  knn.predict(X_test)

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
print(cm)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Assuming X and y are already defined
# X, y = ...

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the KNN model
knn_model = KNeighborsClassifier()

# Define the parameter grid for Grid Search
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')

# Use joblib's parallel_backend to set inner_max_num_threads
with joblib.parallel_backend('loky', inner_max_num_threads=1):
    grid_search.fit(X_train, y_train)

# Get the best parameters from Grid Search
best_params = grid_search.best_params_

# Train the KNN model with the best parameters
best_knn_model = grid_search.best_estimator_
best_knn_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_knn_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=['healthy', 'neuropathy', 'myopathy'])

print(f'Best Parameters: {best_params}')
print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(report)

from google.colab import files
from google.colab import files

import pickle

# Save the model (same as before)
with open('modelknn.pkl', 'wb') as file:
    pickle.dump(knn, file)

# Download the file
#files.download('modelknn.pkl')

y_pred

y_test